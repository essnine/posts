# Large Language Models make me nervous

> Date created: 24-08-2025
> State: Ongoing

Hello, world.

At the time of writing (beginning sometime in June 2025), I've been programming professionally for around seven years. Of those seven, I think the past three have been unprecedented times in terms of how digital computation has evolved. I'm not going to get into scratching out a timeline, because we all know what happened: in 2022, OpenAI made available a free version of ChatGPT to the world.

Since then, there's been... well, there's been a lot going on. 

GenAI has been the thing everyone's been going on about for the past couple of years in a gradually increasing pitch, and for good reason. The capabilities made possible by LLMs are diverse, composable, and well-aligned with the direction that the software development industry was calibrating for: to optimize for software developed in the large. All the patterns build into the development ecosystem for programming languages and tooling that are the go-to tools of the era are focused on development velocity over everything else, with variations on which parameter of the tooling is tweaked to bring in more speed. Programming languages, cloud service design patterns, deployment strategies, fucking... microservices, all aimed towards shipping. Fuck yeah, let's keep shipping.

I don't quite know if we're in that phase of tool adoption where anything beyond the first-order effects of adoption is not in consideration at all. I think we are, but I don't know for sure. It's hard to make blanket statements about an entire industry, especially one where we now have celebrity devs, high-profile programmers who (willingly or otherwise) hold actual influence over the general populace of programmers. It's insane.

I am worried about AI the same way I was worried about social media in kids' lives - the consequences of giving people tools that exceed what they would be capable of without said tools beyond a certain threshold, much the same as letting a newbie motorcyclist ride a machine that's beyond their skill level, or giving teenagers admin access to a computer without firewalls and ad-blocking. It's not about the AI being competent at generating code or whatever, far from it. It's about the user ending up in a situation that they can't get out of.

With the recent developments in AI tooling for programmers, it feels like the interaction paradigm that's winning for programmers is essentially based off of pair programming where humans are permanently in some version of the navigator role. (I don't know if that'll get flipped around at some point, there might be some value in doing so - AI-guided pair programming would be a somewhat effective learning model for new programmers, but I'm not sure if bean counters would find it interesting to spend all that money on licenses for juniors - we're already hearing a lot about fewer junior openings in companies now, I don't see the C-Suite Big Brains of the industry figuring that out without coming across and plagiarizing a LinkedIn post about it first.)

Anyway, that diversion aside, the expectation that AI tooling will increase productivity globally doesn't stand true, simply because the vast majority of programmers aren't as skilled as the biggest proponents of these tools to scrutinize the code-gen output in a meaningful sense. And on my side of the software industry, the unglamorous, unappealing software consultancy side of things, there are few people who are interested in (never mind capable of) scrutiny of any order at all. You'd think all the improved tooling you throw at these teams would improve their output, in some way raising the floor of code quality for them. But the space in which this floor exists isn't two- or three-dimensional. We're simply creating a whole new form of incomprehensible code - code that was generated by AI, reviewed by AI, debugged by AI.

Your government's tax management systems aren't developed by Armin Ronacher or levelsio or whoever else is advocating/testing/hailing/pushing the codegen tool of the hour - are you prepared for that code to end up uncomprehended, if not entirely incomprehensible? Are you sure that the contractor employed by the consultancy that has a billing model based entirely of small-quote-upsell-later is going to have those PR's reviewed, those tests vetted? I know the usual retort to this: they've already done a bad job in the past, might as well try to shake things up and see what sticks - are you prepared for it to get unprecedentedly worse? And not just for you, the well-informed aero-press-sipping 60-percent-board-touch-typing home-row-inhabiting free-of-the-matrix operator. Are you prepared for things getting measurably worse for all the people around you?

Besides, what sort of systems are we really talking about here when it comes to the net impact of AI in shipping stuff?

[TBC]
